#!/usr/bin/env python3
"""
ä¸»è°ƒåº¦è„šæœ¬ - AST-Solidity æµæ°´çº¿å…¥å£
åŠŸèƒ½ï¼šæºä»£ç  -> AST -> DFG -> æ£€æµ‹ -> ç»“æœï¼ˆå¯é€‰å¯è§†åŒ–ï¼‰

ä¼˜åŒ–ç‰ˆæœ¬:
- æ”¯æŒé…ç½®æ–‡ä»¶ç®¡ç†
- æ”¯æŒæ•°æ®é›†æ‰¹é‡å¤„ç†
- å¢å¼ºçš„æ£€æµ‹åŠŸèƒ½(ç¼“å­˜ã€å¹¶å‘æ§åˆ¶)
- Resultç±»å‹çš„å‡½æ•°å¼é”™è¯¯å¤„ç†
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Optional, Dict, Any, List
import time
from datetime import datetime

from .analyzer import SolidityAnalyzer
from .dfg_builder.dfg_config import DFGConfig, OutputMode
from .visualization.visualizer import DFGVisualizer
from .detector.llm_detector import PonziDetectionPipeline, LLMConfig
from .utils.config_manager import PipelineConfig, LLMProviderConfig
from .utils.dataset_loader import DatasetLoader, ContractEntry
from .utils.result import Result


class SolidityPipeline:
    """Solidity æ™ºèƒ½åˆçº¦åˆ†ææµæ°´çº¿ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(
        self,
        config: Optional[PipelineConfig] = None,
        solidity_version: Optional[str] = None,
        output_dir: Optional[str] = None,
        dfg_mode: Optional[OutputMode] = None,
        enable_detection: Optional[bool] = None,
        enable_visualization: Optional[bool] = None,
        llm_config: Optional[LLMConfig] = None
    ):
        """
        åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            config: å®Œæ•´é…ç½®å¯¹è±¡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            solidity_version: Solidityç‰ˆæœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            dfg_mode: DFGè¾“å‡ºæ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            enable_detection: æ˜¯å¦å¯ç”¨åºæ°éª—å±€æ£€æµ‹ï¼ˆå‘åå…¼å®¹ï¼‰
            enable_visualization: æ˜¯å¦å¯ç”¨DFGå¯è§†åŒ–ï¼ˆå‘åå…¼å®¹ï¼‰
            llm_config: LLMé…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        # ä½¿ç”¨é…ç½®å¯¹è±¡æˆ–åˆ›å»ºé»˜è®¤é…ç½®
        if config is None:
            config = PipelineConfig()
            
            # åº”ç”¨å‘åå…¼å®¹å‚æ•°
            if solidity_version:
                config.solidity_version = solidity_version
            if output_dir:
                config.output.output_dir = output_dir
            if dfg_mode:
                config.dfg.mode = dfg_mode.value if hasattr(dfg_mode, 'value') else str(dfg_mode)
            if enable_detection is not None:
                config.detection.enabled = enable_detection
            if llm_config:
                config.detection.provider = LLMProviderConfig(
                    api_key=llm_config.api_key,
                    base_url=llm_config.base_url,
                    model=llm_config.model
                )
        
        self.config = config
        self.enable_visualization = enable_visualization if enable_visualization is not None else False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(config.output.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–DFGé…ç½®
        mode_map = {
            'compact': DFGConfig.compact(),
            'standard': DFGConfig.standard(),
            'verbose': DFGConfig.verbose()
        }
        self.dfg_config = mode_map.get(config.dfg.mode, DFGConfig.standard())
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.analyzer = SolidityAnalyzer(
            solidity_version=config.solidity_version,
            output_dir=str(self.output_dir),
            dfg_config=self.dfg_config
        )
        
        # åˆå§‹åŒ–å¯è§†åŒ–å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.visualizer = DFGVisualizer() if self.enable_visualization else None
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if config.detection.enabled:
            llm_cfg = LLMConfig(
                api_key=config.detection.provider.api_key or "",
                base_url=config.detection.provider.base_url or "",
                model=config.detection.provider.model or ""
            )
            self.detector = PonziDetectionPipeline(config=llm_cfg)
        else:
            self.detector = None
    
    def process_file(self, source_file: Path, contract_name: Optional[str] = None, 
                    label: Optional[int] = None, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªSolidityæºæ–‡ä»¶

        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„
            contract_name: åˆçº¦åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
            label: æ•°æ®é›†æ ‡ç­¾ï¼ˆå¯é€‰ï¼Œ0=åˆæ³•ï¼Œ1=åºæ°éª—å±€ï¼‰
            metadata: é¢å¤–çš„å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        if contract_name is None:
            contract_name = source_file.stem

        print(f"\n{'='*70}")
        print(f"å¤„ç†åˆçº¦: {contract_name}")
        print(f"æºæ–‡ä»¶: {source_file}")
        print(f"{'='*70}")

        result = {
            "contract_name": contract_name,
            "source_file": str(source_file),
            "timestamp": datetime.now().isoformat(),
            "steps": {}
        }

        try:
            # æ­¥éª¤1: è¯»å–æºä»£ç 
            print("\n[1/5] è¯»å–æºä»£ç ...")
            start_time = time.time()
            with open(source_file, 'r', encoding='utf-8') as f:
                source_code = f.read()
            result["steps"]["read_source"] = {
                "status": "success",
                "time": time.time() - start_time,
                "code_length": len(source_code)
            }
            print(f"    âœ… è¯»å–æˆåŠŸ ({len(source_code)} å­—ç¬¦)")

            # æ­¥éª¤2: æ„å»ºAST
            print("\n[2/5] æ„å»ºAST...")
            start_time = time.time()
            
            # å‡†å¤‡é¢å¤–çš„å…ƒæ•°æ®
            extra_metadata = {}
            if label is not None:
                extra_metadata['label'] = label
            if metadata:
                extra_metadata.update(metadata)
            
            # è°ƒç”¨ analyze_source æ–¹æ³•ï¼Œç¦ç”¨å†…ç½®çš„JSONå’Œå¯è§†åŒ–ç”Ÿæˆ
            analysis_result = self.analyzer.analyze_source(
                source_code,
                contract_name,
                generate_json=True,
                generate_visualization=False,  # æˆ‘ä»¬åé¢ä¼šè‡ªå·±ç”Ÿæˆ
                generate_summary=False,
                extra_metadata=extra_metadata if extra_metadata else None
            )

            if not analysis_result.get("success", False):
                error_msg = analysis_result.get("error", "Unknown error")
                result["steps"]["build_ast"] = {
                    "status": "failed",
                    "error": error_msg,
                    "time": time.time() - start_time
                }
                print(f"    âŒ ASTæ„å»ºå¤±è´¥: {error_msg}")
                result["status"] = "failed"
                result["error"] = error_msg
                return result

            result["steps"]["build_ast"] = {
                "status": "success",
                "time": time.time() - start_time,
                "ast_nodes": analysis_result.get("ast_nodes", 0)
            }
            print(f"    âœ… ASTæ„å»ºæˆåŠŸ")
            print(f"       ASTèŠ‚ç‚¹æ•°: {analysis_result.get('ast_nodes', 0)}")

            # æ­¥éª¤3: æ„å»ºDFG
            print("\n[3/5] æ„å»ºDFG...")
            start_time = time.time()
            dfg_json_path = analysis_result.get("json_file")

            if dfg_json_path and Path(dfg_json_path).exists():
                result["steps"]["build_dfg"] = {
                    "status": "success",
                    "time": time.time() - start_time,
                    "output_file": str(dfg_json_path),
                    "node_count": analysis_result.get("dfg_nodes", 0),
                    "edge_count": analysis_result.get("dfg_edges", 0),
                    "filtered_nodes": analysis_result.get("filtered_nodes", 0),
                    "filtered_edges": analysis_result.get("filtered_edges", 0)
                }
                print(f"    âœ… DFGæ„å»ºæˆåŠŸ")
                print(f"       èŠ‚ç‚¹æ•°: {analysis_result.get('dfg_nodes', 0)}")
                print(f"       è¾¹æ•°: {analysis_result.get('dfg_edges', 0)}")
                print(f"       è¿‡æ»¤èŠ‚ç‚¹: {analysis_result.get('filtered_nodes', 0)}")
                print(f"       è¾“å‡º: {dfg_json_path}")
            else:
                result["steps"]["build_dfg"] = {
                    "status": "failed",
                    "error": "DFGæ„å»ºå¤±è´¥"
                }
                print(f"    âŒ DFGæ„å»ºå¤±è´¥")
                return result

            # æ­¥éª¤4: åºæ°éª—å±€æ£€æµ‹ï¼ˆå¯é€‰ï¼‰
            if self.config.detection.enabled and self.detector:
                print("\n[4/5] æ‰§è¡Œåºæ°éª—å±€æ£€æµ‹...")
                start_time = time.time()
                try:
                    # è¯»å–DFG JSONè¿›è¡Œæ£€æµ‹
                    import asyncio
                    detection_result = asyncio.run(
                        self.detector.detect_from_json_file(str(dfg_json_path))
                    )

                    result["steps"]["detection"] = {
                        "status": "success",
                        "time": time.time() - start_time,
                        "result": detection_result
                    }

                    # ä¿å­˜æ£€æµ‹ç»“æœ
                    detection_output = self.output_dir / f"{contract_name}_detection.json"
                    with open(detection_output, 'w', encoding='utf-8') as f:
                        json.dump(detection_result, f, indent=2, ensure_ascii=False)

                    classification = detection_result.get("classification_result", {})
                    print(f"    âœ… æ£€æµ‹å®Œæˆ")
                    print(f"       æ˜¯å¦ä¸ºåºæ°éª—å±€: {classification.get('is_ponzi', False)}")
                    print(f"       ç½®ä¿¡åº¦: {classification.get('confidence', 0.0):.2f}")
                    print(f"       é£é™©ç­‰çº§: {classification.get('risk_level', 'æœªçŸ¥')}")
                    print(f"       ç»“æœä¿å­˜è‡³: {detection_output}")

                except Exception as e:
                    result["steps"]["detection"] = {
                        "status": "failed",
                        "error": str(e),
                        "time": time.time() - start_time
                    }
                    print(f"    âŒ æ£€æµ‹å¤±è´¥: {e}")
            else:
                print("\n[4/5] è·³è¿‡æ£€æµ‹ï¼ˆæœªå¯ç”¨ï¼‰")
                result["steps"]["detection"] = {"status": "skipped"}

            # æ­¥éª¤5: å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
            if self.enable_visualization and self.visualizer:
                print("\n[5/5] ç”ŸæˆDFGå¯è§†åŒ–...")
                start_time = time.time()
                try:
                    viz_output = self.output_dir / f"{contract_name}_dfg"
                    success = self.visualizer.visualize_from_json(
                        str(dfg_json_path),
                        str(viz_output)
                    )
                    if success:
                        result["steps"]["visualization"] = {
                            "status": "success",
                            "time": time.time() - start_time,
                            "output_file": f"{viz_output}.png"
                        }
                        print(f"    âœ… å¯è§†åŒ–å®Œæˆ")
                        print(f"       è¾“å‡º: {viz_output}.png")
                    else:
                        result["steps"]["visualization"] = {
                            "status": "failed",
                            "error": "å¯è§†åŒ–ç”Ÿæˆå¤±è´¥",
                            "time": time.time() - start_time
                        }
                        print(f"    âŒ å¯è§†åŒ–å¤±è´¥")
                except Exception as e:
                    result["steps"]["visualization"] = {
                        "status": "failed",
                        "error": str(e),
                        "time": time.time() - start_time
                    }
                    print(f"    âŒ å¯è§†åŒ–å¤±è´¥: {e}")
            else:
                print("\n[5/5] è·³è¿‡å¯è§†åŒ–ï¼ˆæœªå¯ç”¨ï¼‰")
                result["steps"]["visualization"] = {"status": "skipped"}

            result["status"] = "success"

        except Exception as e:
            result["status"] = "failed"
            result["error"] = str(e)
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        return result

    def process_batch(self, source_files: List[Path]) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªSolidityæºæ–‡ä»¶

        Args:
            source_files: æºæ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            æ‰¹å¤„ç†ç»“æœå­—å…¸
        """
        print(f"\n{'='*70}")
        print(f"æ‰¹é‡å¤„ç†æ¨¡å¼")
        print(f"æ–‡ä»¶æ€»æ•°: {len(source_files)}")
        print(f"{'='*70}")

        batch_result = {
            "total_files": len(source_files),
            "timestamp": datetime.now().isoformat(),
            "results": [],
            "summary": {
                "success": 0,
                "failed": 0,
                "total_time": 0
            }
        }

        start_time = time.time()

        for i, source_file in enumerate(source_files, 1):
            print(f"\nè¿›åº¦: [{i}/{len(source_files)}]")
            result = self.process_file(source_file)
            batch_result["results"].append(result)

            if result["status"] == "success":
                batch_result["summary"]["success"] += 1
            else:
                batch_result["summary"]["failed"] += 1

        batch_result["summary"]["total_time"] = time.time() - start_time

        # ä¿å­˜æ‰¹å¤„ç†ç»“æœ
        batch_output = self.output_dir / f"batch_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_output, 'w', encoding='utf-8') as f:
            json.dump(batch_result, f, indent=2, ensure_ascii=False)

        # æ‰“å°æ€»ç»“
        print(f"\n{'='*70}")
        print(f"æ‰¹å¤„ç†å®Œæˆ")
        print(f"{'='*70}")
        print(f"æ€»æ–‡ä»¶æ•°: {batch_result['total_files']}")
        print(f"æˆåŠŸ: {batch_result['summary']['success']}")
        print(f"å¤±è´¥: {batch_result['summary']['failed']}")
        print(f"æ€»è€—æ—¶: {batch_result['summary']['total_time']:.2f}ç§’")
        print(f"ç»“æœä¿å­˜è‡³: {batch_output}")
        print(f"{'='*70}")
        
        return batch_result
    
    def process_dataset(self, dataset_path: str, limit: Optional[int] = None) -> Result[Dict[str, Any]]:
        """
        ä»æ•°æ®é›†æ–‡ä»¶æ‰¹é‡å¤„ç†åˆçº¦
        
        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
            limit: å¯é€‰çš„å¤„ç†æ•°é‡é™åˆ¶
            
        Returns:
            ResultåŒ…å«å¤„ç†ç»“æœå­—å…¸
        """
        print(f"\n{'='*70}")
        print(f"æ•°æ®é›†å¤„ç†æ¨¡å¼")
        print(f"æ•°æ®é›†æ–‡ä»¶: {dataset_path}")
        if limit:
            print(f"é™åˆ¶æ•°é‡: {limit}")
        print(f"{'='*70}")
        
        # åŠ è½½æ•°æ®é›†
        load_result = DatasetLoader.load_json_dataset(dataset_path)
        if load_result.is_failure:
            print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {load_result.error}")
            return Result.failure(load_result.error)
        
        entries = load_result.value
        
        # åº”ç”¨é™åˆ¶
        if limit and limit > 0:
            entries = entries[:limit]
        
        print(f"\nå°†å¤„ç† {len(entries)} ä¸ªåˆçº¦...")
        
        batch_result = {
            "dataset": dataset_path,
            "total_entries": len(entries),
            "timestamp": datetime.now().isoformat(),
            "results": [],
            "summary": {
                "success": 0,
                "failed": 0,
                "total_time": 0
            }
        }
        
        start_time = time.time()
        
        for i, entry in enumerate(entries, 1):
            contract_name = f"contract_{entry.index}"
            if entry.metadata and 'file_name' in entry.metadata:
                contract_name = entry.metadata['file_name'].replace('.sol', '')
            
            print(f"\nè¿›åº¦: [{i}/{len(entries)}] å¤„ç† {contract_name}...")
            
            try:
                # å°†ä»£ç å†™å…¥ä¸´æ—¶æ–‡ä»¶
                temp_file = self.output_dir / f"temp_{contract_name}.sol"
                with open(temp_file, 'w', encoding='utf-8') as f:
                    f.write(entry.code)
                
                # å¤„ç†æ–‡ä»¶ï¼Œä¼ é€’labelå’Œmetadata
                result = self.process_file(temp_file, contract_name, 
                                          label=entry.label, 
                                          metadata=entry.metadata)
                
                # æ·»åŠ æ ‡ç­¾åˆ°ç»“æœï¼ˆç”¨äºæ‰¹å¤„ç†æŠ¥å‘Šï¼‰
                if entry.label is not None:
                    result["label"] = entry.label
                if entry.metadata:
                    result["metadata"] = entry.metadata
                
                batch_result["results"].append(result)
                
                if result["status"] == "success":
                    batch_result["summary"]["success"] += 1
                    print(f"  âœ… æˆåŠŸ")
                else:
                    batch_result["summary"]["failed"] += 1
                    print(f"  âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file.exists():
                    temp_file.unlink()
                    
            except Exception as e:
                error_result = {
                    "contract_name": contract_name,
                    "status": "failed",
                    "error": str(e),
                    "label": entry.label
                }
                batch_result["results"].append(error_result)
                batch_result["summary"]["failed"] += 1
                print(f"  âŒ å¼‚å¸¸: {e}")
        
        batch_result["summary"]["total_time"] = time.time() - start_time
        
        # ä¿å­˜æ‰¹å¤„ç†ç»“æœ
        batch_output = self.output_dir / f"dataset_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(batch_output, 'w', encoding='utf-8') as f:
            json.dump(batch_result, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ€»ç»“
        print(f"\n{'='*70}")
        print(f"æ•°æ®é›†å¤„ç†å®Œæˆ")
        print(f"{'='*70}")
        print(f"æ€»æ¡ç›®æ•°: {batch_result['total_entries']}")
        print(f"æˆåŠŸ: {batch_result['summary']['success']}")
        print(f"å¤±è´¥: {batch_result['summary']['failed']}")
        print(f"æ€»è€—æ—¶: {batch_result['summary']['total_time']:.2f}ç§’")
        print(f"ç»“æœä¿å­˜è‡³: {batch_output}")
        print(f"{'='*70}")
        
        return Result.success(batch_result)
def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="Solidity æ™ºèƒ½åˆçº¦åˆ†ææµæ°´çº¿",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åˆ†æå•ä¸ªåˆçº¦æ–‡ä»¶
  python -m src.main contract.sol
  
  # ä»é…ç½®æ–‡ä»¶åŠ è½½è®¾ç½®
  python -m src.main contract.sol --config config.json
  
  # å¤„ç†JSONæ•°æ®é›†
  python -m src.main --dataset data.json
  python -m src.main --dataset data.json --limit 100

  # åˆ†æå¹¶æ£€æµ‹åºæ°éª—å±€
  python -m src.main contract.sol --detect
  
  # æ‰¹é‡æ£€æµ‹å·²ç”Ÿæˆçš„JSONæ–‡ä»¶
  python -m src.main --detect-only --detect-dir output

  # åˆ†æã€æ£€æµ‹å¹¶å¯è§†åŒ–
  python -m src.main contract.sol --detect --visualize

  # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶
  python -m src.main file1.sol file2.sol file3.sol --batch

  # ä½¿ç”¨ç´§å‡‘æ¨¡å¼è¾“å‡º
  python -m src.main contract.sol --mode compact

  # æŒ‡å®šLLMæä¾›å•†
  python -m src.main contract.sol --detect --llm-provider deepseek
        """
    )

    # åŸºæœ¬å‚æ•°
    parser.add_argument(
        'files',
        nargs='*',  # æ”¹ä¸ºå¯é€‰ï¼Œæ”¯æŒ --dataset æ¨¡å¼
        type=str,
        help='Solidityæºæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šä¸ªæ–‡ä»¶ï¼‰'
    )
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument(
        '--dataset',
        type=str,
        help='JSONæ•°æ®é›†æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        help='é™åˆ¶å¤„ç†çš„åˆçº¦æ•°é‡'
    )
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument(
        '--config',
        type=str,
        help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰'
    )

    parser.add_argument(
        '-o', '--output',
        type=str,
        default='output',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputï¼‰'
    )

    parser.add_argument(
        '--solidity-version',
        type=str,
        default='0.4.x',
        choices=['0.4.x', '0.5.x', '0.6.x', '0.7.x', '0.8.x'],
        help='Solidityç‰ˆæœ¬ï¼ˆé»˜è®¤: 0.4.xï¼‰'
    )

    # DFGé…ç½®
    parser.add_argument(
        '-m', '--mode',
        type=str,
        default='standard',
        choices=['compact', 'standard', 'verbose'],
        help='DFGè¾“å‡ºæ¨¡å¼ï¼ˆé»˜è®¤: standardï¼‰'
    )

    # åŠŸèƒ½å¼€å…³
    parser.add_argument(
        '-d', '--detect',
        action='store_true',
        help='å¯ç”¨åºæ°éª—å±€æ£€æµ‹'
    )
    
    parser.add_argument(
        '--detect-only',
        action='store_true',
        help='ä»…è¿è¡Œæ£€æµ‹ï¼Œä¸ç”ŸæˆDFGï¼ˆéœ€è¦å·²æœ‰JSONæ–‡ä»¶ï¼‰'
    )
    
    parser.add_argument(
        '--detect-dir',
        type=str,
        default='output',
        help='æ£€æµ‹æ¨¡å¼çš„è¾“å…¥ç›®å½•ï¼ˆé»˜è®¤: outputï¼‰'
    )
    
    parser.add_argument(
        '--detect-pattern',
        type=str,
        default='*.json',
        help='æ£€æµ‹æ¨¡å¼çš„æ–‡ä»¶æ¨¡å¼ï¼ˆé»˜è®¤: *.jsonï¼‰'
    )

    parser.add_argument(
        '--visualize',
        action='store_true',
        help='å¯ç”¨DFGå¯è§†åŒ–'
    )

    parser.add_argument(
        '-b', '--batch',
        action='store_true',
        help='æ‰¹é‡å¤„ç†æ¨¡å¼'
    )

    # LLMé…ç½®ï¼ˆç”¨äºæ£€æµ‹ï¼‰
    parser.add_argument(
        '--llm-provider',
        type=str,
        choices=['qwen', 'deepseek', 'openai'],
        default='qwen',
        help='LLMæä¾›å•†ï¼ˆé»˜è®¤: qwenï¼‰'
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        help='LLM APIå¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰'
    )

    parser.add_argument(
        '--base-url',
        type=str,
        help='LLM APIåŸºç¡€URLï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰'
    )

    parser.add_argument(
        '--model',
        type=str,
        help='LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰'
    )
    
    parser.add_argument(
        '--concurrency',
        type=int,
        default=40,
        help='æ‰¹é‡æ£€æµ‹å¹¶å‘æ•°ï¼ˆé»˜è®¤: 40ï¼‰'
    )
    
    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='ç¦ç”¨æ£€æµ‹ç»“æœç¼“å­˜'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è¯¦ç»†è¾“å‡ºæ¨¡å¼'
    )

    args = parser.parse_args()
    
    # æ£€æµ‹ä»…æ£€æµ‹æ¨¡å¼
    if args.detect_only:
        import asyncio
        from .detector.batch_detector import BatchDetector
        
        print("\nğŸ” ä»…æ£€æµ‹æ¨¡å¼ - å¤„ç†å·²æœ‰JSONæ–‡ä»¶")
        
        # ä»é…ç½®æˆ–å‚æ•°åˆ›å»ºLLMé…ç½®
        if args.config:
            config = PipelineConfig.load_from_file(args.config)
            config = PipelineConfig.load_from_args(args)
        else:
            config = PipelineConfig.load_from_args(args)
            config.detection.enabled = True
        
        llm_config = LLMConfig(
            api_key=config.detection.provider.api_key or "",
            base_url=config.detection.provider.base_url or "",
            model=config.detection.provider.model or ""
        )
        
        detector = BatchDetector(
            config=llm_config,
            concurrency_limit=args.concurrency,
            cache_dir="cache",
            enable_cache=not args.no_cache
        )
        
        async def run_detection():
            stats = await detector.detect_batch(
                output_dir=args.detect_dir,
                pattern=args.detect_pattern,
                limit=args.limit
            )
            detector.print_statistics(stats)
            detector.save_results(stats)
            return stats['successful'] > 0
        
        success = asyncio.run(run_detection())
        sys.exit(0 if success else 1)
    
    # æ•°æ®é›†å¤„ç†æ¨¡å¼
    if args.dataset:
        # åŠ è½½é…ç½®
        if args.config:
            config = PipelineConfig.load_from_file(args.config)
            config = PipelineConfig.load_from_args(args)
        else:
            config = PipelineConfig.load_from_args(args)
        
        # åˆ›å»ºæµæ°´çº¿
        pipeline = SolidityPipeline(config=config)
        
        # å¤„ç†æ•°æ®é›†
        result = pipeline.process_dataset(args.dataset, args.limit)
        
        if result.is_success:
            summary = result.value["summary"]
            sys.exit(0 if summary["success"] > 0 else 1)
        else:
            print(f"\nâŒ æ•°æ®é›†å¤„ç†å¤±è´¥: {result.error}")
            sys.exit(1)
    
    # å¸¸è§„æ–‡ä»¶å¤„ç†æ¨¡å¼
    if not args.files:
        print("é”™è¯¯: è¯·æä¾›Solidityæºæ–‡ä»¶æˆ–ä½¿ç”¨ --dataset å‚æ•°")
        sys.exit(1)
    
    # åŠ è½½é…ç½®
    if args.config:
        config = PipelineConfig.load_from_file(args.config)
        config = PipelineConfig.load_from_args(args)
    else:
        config = PipelineConfig.load_from_args(args)
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = SolidityPipeline(config=config, enable_visualization=args.visualize)

    # å¤„ç†æ–‡ä»¶
    source_files = [Path(f) for f in args.files]

    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    for f in source_files:
        if not f.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {f}")
            sys.exit(1)

    # æ‰¹é‡æ¨¡å¼æˆ–å•æ–‡ä»¶æ¨¡å¼
    if args.batch or len(source_files) > 1:
        result = pipeline.process_batch(source_files)
        # æ‰¹é‡å¤„ç†çš„æˆåŠŸåˆ¤æ–­
        if result.get("summary", {}).get("success", 0) > 0:
            sys.exit(0)
        else:
            sys.exit(1)
    else:
        result = pipeline.process_file(source_files[0])
        # å•æ–‡ä»¶å¤„ç†çš„æˆåŠŸåˆ¤æ–­
        if result.get("status") == "success":
            sys.exit(0)
        else:
            sys.exit(1)


if __name__ == "__main__":
    main()
