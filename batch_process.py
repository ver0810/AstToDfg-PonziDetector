#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†JSONæ–‡ä»¶ä¸­çš„Solidityåˆçº¦,ç”ŸæˆDFGè€Œä¸ç”Ÿæˆå¯è§†åŒ–
"""

import json
import os
import sys
from pathlib import Path
import time
from datetime import datetime
from typing import Dict, List, Tuple

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.analyzer import SolidityAnalyzer
from src.dfg_config import DFGConfig, OutputMode


class BatchProcessor:
    """æ‰¹é‡å¤„ç†Solidityåˆçº¦çš„DFGç”Ÿæˆå™¨"""
    
    def __init__(self, input_file: str, output_dir: str, mode: OutputMode = OutputMode.STANDARD):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            input_file: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            mode: DFGè¾“å‡ºæ¨¡å¼ (COMPACT/STANDARD/VERBOSE)
        """
        self.input_file = input_file
        self.output_dir = Path(output_dir)
        self.mode = mode
        
        # ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼š20251021_143229ï¼‰
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'total_nodes_before': 0,
            'total_nodes_after': 0,
            'total_time': 0.0
        }
        
        # é”™è¯¯è®°å½•
        self.errors: List[Tuple[int, str]] = []
        
    def load_contracts(self) -> List[Dict]:
        """
        ä»JSONLæ–‡ä»¶åŠ è½½åˆçº¦
        
        Returns:
            åˆçº¦åˆ—è¡¨
        """
        contracts = []
        print(f"ğŸ“– æ­£åœ¨åŠ è½½åˆçº¦æ•°æ®: {self.input_file}")
        
        with open(self.input_file, 'r', encoding='utf-8') as f:
            for line_no, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    contracts.append({
                        'line_no': line_no,
                        'code': data.get('code', ''),
                        'label': data.get('label', 0)
                    })
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è­¦å‘Š: ç¬¬{line_no}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    self.stats['skipped'] += 1
                    
        print(f"âœ… æˆåŠŸåŠ è½½ {len(contracts)} ä¸ªåˆçº¦")
        return contracts
    
    def process_contract(self, contract_data: Dict, index: int) -> bool:
        """
        å¤„ç†å•ä¸ªåˆçº¦
        
        Args:
            contract_data: åˆçº¦æ•°æ® {'line_no', 'code', 'label'}
            index: åˆçº¦ç´¢å¼•
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        line_no = contract_data['line_no']
        code = contract_data['code']
        label = contract_data['label']
        
        # è·³è¿‡ç©ºä»£ç 
        if not code or not code.strip():
            self.stats['skipped'] += 1
            return False
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            temp_file = self.output_dir / f"temp_contract_{index}.sol"
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(code)
            
            # åˆ›å»ºé…ç½®
            mode_config_map = {
                OutputMode.COMPACT: DFGConfig.compact(),
                OutputMode.STANDARD: DFGConfig.standard(),
                OutputMode.VERBOSE: DFGConfig.verbose()
            }
            config = mode_config_map[self.mode]
            
            # åˆ†æåˆçº¦ï¼ˆä¼ å…¥é…ç½®ï¼‰
            analyzer = SolidityAnalyzer(
                solidity_version="0.4.x",
                output_dir=str(self.output_dir),
                dfg_config=config
            )
            
            # æ„å»ºAST
            ast_root = analyzer.ast_builder.build_ast(code)
            if not ast_root:
                self.errors.append((line_no, "Failed to build AST"))
                temp_file.unlink()
                return False
            
            # æå–åˆçº¦åç§°
            contract_name = analyzer._extract_contract_name(ast_root) or f"Contract_{index}"
            
            # å¤„ç†0.4.xç‰¹æ€§
            if analyzer.legacy_handler:
                analyzer._process_legacy_features(ast_root)
            
            # æ„å»ºDFG
            dfg = analyzer.dfg_builder.build_dfg(ast_root, contract_name)
            if not dfg:
                self.errors.append((line_no, "Failed to build DFG"))
                temp_file.unlink()
                return False
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            temp_file.unlink()
            
            # ç”Ÿæˆæ–‡ä»¶åï¼šcontract_{ç´¢å¼•}_{æ—¶é—´æˆ³}
            # ç´¢å¼•ä»0å¼€å§‹ï¼ˆindex-1ï¼‰ï¼Œå› ä¸ºindexä»1å¼€å§‹
            output_file = self.output_dir / f"contract_{index-1}_{self.timestamp}.json"
            
            # ä½¿ç”¨JSONSerializerä¿å­˜
            dfg_json = analyzer.json_serializer.serialize_dfg(dfg)
            
            # æ·»åŠ å…ƒæ•°æ®
            dfg_json['metadata'] = {
                'index': index - 1,  # ä»0å¼€å§‹çš„ç´¢å¼•
                'source_line': line_no,
                'label': label,
                'mode': self.mode.value,
                'contract_name': contract_name,
                'timestamp': self.timestamp
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(dfg_json, f, indent=2, ensure_ascii=False)
            
            # æ›´æ–°ç»Ÿè®¡
            ast_node_count = analyzer._count_ast_nodes(ast_root)
            self.stats['total_nodes_before'] += ast_node_count
            self.stats['total_nodes_after'] += len(dfg.nodes)
            
            return True
            
        except Exception as e:
            error_msg = f"ç¬¬{line_no}è¡Œå¤„ç†å¤±è´¥: {str(e)}"
            self.errors.append((line_no, str(e)))
            return False
    
    def process_all(self, progress_interval: int = 100):
        """
        å¤„ç†æ‰€æœ‰åˆçº¦
        
        Args:
            progress_interval: æ¯å¤„ç†å¤šå°‘ä¸ªåˆçº¦æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        """
        contracts = self.load_contracts()
        self.stats['total'] = len(contracts)
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† (æ¨¡å¼: {self.mode.value})")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"=" * 60)
        
        start_time = time.time()
        
        for idx, contract in enumerate(contracts, 1):
            # æ˜¾ç¤ºè¿›åº¦
            if idx % progress_interval == 0 or idx == 1:
                elapsed = time.time() - start_time
                rate = idx / elapsed if elapsed > 0 else 0
                eta = (len(contracts) - idx) / rate if rate > 0 else 0
                
                print(f"ğŸ“Š è¿›åº¦: {idx}/{len(contracts)} ({idx*100//len(contracts)}%) | "
                      f"æˆåŠŸ: {self.stats['success']} | "
                      f"å¤±è´¥: {self.stats['failed']} | "
                      f"é€Ÿç‡: {rate:.1f} contracts/s | "
                      f"é¢„è®¡å‰©ä½™: {eta/60:.1f}åˆ†é’Ÿ")
            
            # å¤„ç†åˆçº¦
            success = self.process_contract(contract, idx)
            
            if success:
                self.stats['success'] += 1
            else:
                self.stats['failed'] += 1
        
        self.stats['total_time'] = time.time() - start_time
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        self.print_summary()
        
        # ä¿å­˜é”™è¯¯æ—¥å¿—
        if self.errors:
            self.save_error_log()
    
    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print(f"\n{'='*60}")
        print("ğŸ“ˆ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ€»åˆçº¦æ•°:     {self.stats['total']}")
        print(f"æˆåŠŸå¤„ç†:     {self.stats['success']} ({self.stats['success']*100//self.stats['total']}%)")
        print(f"å¤„ç†å¤±è´¥:     {self.stats['failed']}")
        print(f"è·³è¿‡:         {self.stats['skipped']}")
        print(f"æ€»è€—æ—¶:       {self.stats['total_time']:.1f} ç§’")
        print(f"å¹³å‡é€Ÿç‡:     {self.stats['total']/(self.stats['total_time'] or 1):.2f} contracts/s")
        
        if self.stats['total_nodes_before'] > 0:
            reduction = (1 - self.stats['total_nodes_after'] / self.stats['total_nodes_before']) * 100
            print(f"\nğŸ¯ èŠ‚ç‚¹ä¼˜åŒ–ç»Ÿè®¡:")
            print(f"ä¼˜åŒ–å‰èŠ‚ç‚¹æ€»æ•°: {self.stats['total_nodes_before']}")
            print(f"ä¼˜åŒ–åèŠ‚ç‚¹æ€»æ•°: {self.stats['total_nodes_after']}")
            print(f"èŠ‚ç‚¹å‡å°‘ç‡:     {reduction:.1f}%")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"{'='*60}")
    
    def save_error_log(self):
        """ä¿å­˜é”™è¯¯æ—¥å¿—"""
        error_log = self.output_dir / "errors.log"
        
        with open(error_log, 'w', encoding='utf-8') as f:
            f.write(f"æ‰¹é‡å¤„ç†é”™è¯¯æ—¥å¿—\n")
            f.write(f"{'='*60}\n")
            f.write(f"æ€»é”™è¯¯æ•°: {len(self.errors)}\n\n")
            
            for line_no, error in self.errors:
                f.write(f"è¡Œå· {line_no}: {error}\n")
        
        print(f"âš ï¸  é”™è¯¯æ—¥å¿—å·²ä¿å­˜: {error_log}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡å¤„ç†Solidityåˆçº¦ç”ŸæˆDFG',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨æ ‡å‡†æ¨¡å¼å¤„ç†
  python batch_process.py data/ponzi_code_dataset_small_1514.json output/batch_dfgs
  
  # ä½¿ç”¨ç´§å‡‘æ¨¡å¼å¤„ç†
  python batch_process.py data/ponzi_code_dataset_small_1514.json output/batch_dfgs --mode compact
  
  # ä½¿ç”¨è¯¦ç»†æ¨¡å¼å¤„ç†
  python batch_process.py data/ponzi_code_dataset_small_1514.json output/batch_dfgs --mode verbose
        """
    )
    
    parser.add_argument('input_file', help='è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('output_dir', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument(
        '--mode',
        choices=['compact', 'standard', 'verbose'],
        default='standard',
        help='DFGè¾“å‡ºæ¨¡å¼ (é»˜è®¤: standard)'
    )
    parser.add_argument(
        '--progress',
        type=int,
        default=100,
        help='è¿›åº¦æ˜¾ç¤ºé—´éš” (é»˜è®¤: æ¯100ä¸ªåˆçº¦)'
    )
    
    args = parser.parse_args()
    
    # è½¬æ¢æ¨¡å¼
    mode_map = {
        'compact': OutputMode.COMPACT,
        'standard': OutputMode.STANDARD,
        'verbose': OutputMode.VERBOSE
    }
    mode = mode_map[args.mode]
    
    # åˆ›å»ºå¤„ç†å™¨å¹¶æ‰§è¡Œ
    processor = BatchProcessor(args.input_file, args.output_dir, mode)
    processor.process_all(progress_interval=args.progress)


if __name__ == '__main__':
    main()
